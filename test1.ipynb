{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Chinese_sim is only compatible with English, try lang_list=[\"ch_sim\",\"en\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01measyocr\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcv\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43measyocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mko\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mch_sim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mja\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadtext(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mex\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprj_poster\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtitle_detect\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcombined_edges.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\easyocr\\easyocr.py:124\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, lang_list, gpu, model_storage_directory, user_network_directory, detect_network, recog_network, download_enabled, detector, recognizer, verbose, quantize, cudnn_benchmark)\u001b[0m\n\u001b[0;32m    122\u001b[0m     recog_network \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mch_sim\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m lang_list:\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetModelLanguage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchinese_sim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mch_sim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mch_sim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     model \u001b[38;5;241m=\u001b[39m recognition_models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen2\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh_sim_g2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    126\u001b[0m     recog_network \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration2\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\easyocr\\easyocr.py:286\u001b[0m, in \u001b[0;36mReader.setModelLanguage\u001b[1;34m(self, language, lang_list, list_lang, list_lang_string)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mch_tra\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mch_sim\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    285\u001b[0m     language \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchinese\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(language\u001b[38;5;241m.\u001b[39mcapitalize() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only compatible with English, try lang_list=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m list_lang_string)\n",
      "\u001b[1;31mValueError\u001b[0m: Chinese_sim is only compatible with English, try lang_list=[\"ch_sim\",\"en\"]"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2 as cv\n",
    "\n",
    "reader = easyocr.Reader(['ko', 'en', 'ch_sim', 'ja'])\n",
    "result = reader.readtext(r'C:\\ex\\prj_poster\\title_detect\\combined_edges.jpg')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected text: 모조관, Confidence: 0.026382442851082842\n",
      "Detected text: [ 제내헤움[다[, Confidence: 0.05200601536962351\n",
      "Detected text: 떼기로록, Confidence: 0.03408586233854294\n",
      "Detected text: 노끗, Confidence: 0.0022432237705254424\n",
      "Detected text: '정어액 슬쩍 , Confidence: 0.0022394028965984697\n",
      "Detected text: \"Plupafiiil, Confidence: 0.0018044526903874417\n",
      "Detected text: 도중;, Confidence: 0.1965672533314896\n",
      "Detected text: K매떨, Confidence: 0.013250079912947623\n",
      "Detected text: nn동 I, Confidence: 0.008182931881100967\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "# 올바른 언어 코드 사용\n",
    "reader = easyocr.Reader(['ko', 'en'])  # 'zh' 사용\n",
    "\n",
    "# 텍스트 추출할 이미지 파일 경로\n",
    "image_path = r'C:\\\\ex\\\\prj_poster\\\\title_detect\\\\combined_edges.jpg'\n",
    "\n",
    "# 이미지에서 텍스트 추출\n",
    "results = reader.readtext(image_path)\n",
    "\n",
    "# 결과 출력\n",
    "for (bbox, text, prob) in results:\n",
    "    print(f\"Detected text: {text}, Confidence: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 이미지 파일 경로\n",
    "image_path = 'img.jpg'\n",
    "\n",
    "# 이미지 로드\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# 1. 가우시안 블러\n",
    "blurred = cv2.GaussianBlur(image, (5, 5), 0)  # 커널 크기를 (5, 5)로 설정\n",
    "\n",
    "# 2. 소벨 엣지 검출\n",
    "sobel_x = cv2.Sobel(blurred, cv2.CV_64F, 1, 0, ksize=3)  # x 방향 소벨 필터, 커널 크기 3\n",
    "sobel_y = cv2.Sobel(blurred, cv2.CV_64F, 0, 1, ksize=3)  # y 방향 소벨 필터\n",
    "sobel_edges = cv2.magnitude(sobel_x, sobel_y)  # 소벨 엣지 결합\n",
    "sobel_edges = np.uint8(sobel_edges)  # 정수형으로 변환\n",
    "\n",
    "# 3. 캐니 엣지 검출\n",
    "threshold1 = 50  # 첫 번째 임계값\n",
    "threshold2 = 150  # 두 번째 임계값\n",
    "canny_edges = cv2.Canny(sobel_edges, threshold1, threshold2)  # 캐니 엣지 검출\n",
    "\n",
    "# 4. 이미지 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(sobel_edges, cmap='gray')\n",
    "plt.title('Sobel Edges')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(canny_edges, cmap='gray')\n",
    "plt.title('Canny Edges')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# EAST 모델을 통한 텍스트 감지\n",
    "east_model_path = \"frozen_east_text_detection.pb\"\n",
    "\n",
    "# 입력 크기 설정\n",
    "input_width = 320\n",
    "input_height = 320\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# 블롭으로 변환하기 전에 컬러 형식 변환\n",
    "canny_edges_color = cv2.cvtColor(canny_edges, cv2.COLOR_GRAY2BGR)  # 그레이스케일을 RGB로 변환\n",
    "\n",
    "# 블롭으로 변환\n",
    "blob = cv2.dnn.blobFromImage(canny_edges_color, 1.0, (input_width, input_height),\n",
    "                             (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "\n",
    "# EAST 모델 로드\n",
    "net = cv2.dnn.readNet(east_model_path)\n",
    "\n",
    "# 입력을 네트워크에 설정\n",
    "net.setInput(blob)\n",
    "\n",
    "# 네트워크의 출력 레이어 정의\n",
    "layer_names = [\n",
    "    \"feature_fusion/Conv_7/Sigmoid\",  # 텍스트 확률 지도\n",
    "    \"feature_fusion/concat_3\"         # 텍스트 경계 박스 좌표\n",
    "]\n",
    "\n",
    "# 모델 추론\n",
    "(scores, geometry) = net.forward(layer_names)\n",
    "\n",
    "# 텍스트 상자 계산 함수\n",
    "def decode_predictions(scores, geometry, conf_threshold):\n",
    "    (num_rows, num_cols) = scores.shape[2:4]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    for y in range(num_rows):\n",
    "        scores_data = scores[0, 0, y]\n",
    "        x0_data = geometry[0, 0, y]\n",
    "        x1_data = geometry[0, 1, y]\n",
    "        x2_data = geometry[0, 2, y]\n",
    "        x3_data = geometry[0, 3, y]\n",
    "        angles_data = geometry[0, 4, y]\n",
    "\n",
    "        for x in range(num_cols):\n",
    "            if scores_data[x] < conf_threshold:\n",
    "                continue\n",
    "\n",
    "            angle = angles_data[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "\n",
    "            h = x0_data[x] + x2_data[x]\n",
    "            w = x1_data[x] + x3_data[x]\n",
    "\n",
    "            end_x = int(x * 4.0 + (cos * x1_data[x]) + (sin * x2_data[x]))\n",
    "            end_y = int(y * 4.0 - (sin * x1_data[x]) + (cos * x2_data[x]))\n",
    "            start_x = int(end_x - w)\n",
    "            start_y = int(end_y - h)\n",
    "\n",
    "            # 상자 형식을 (start_x, start_y, width, height)로 변환\n",
    "            boxes.append([start_x, start_y, int(w), int(h)])\n",
    "            confidences.append(float(scores_data[x]))\n",
    "\n",
    "    return (boxes, confidences)\n",
    "\n",
    "# 감지된 텍스트 상자 및 신뢰도 값 계산\n",
    "(boxes, confidences) = decode_predictions(scores, geometry, conf_threshold)\n",
    "\n",
    "# NMS (Non-Maximum Suppression)를 통해 겹치는 박스 제거\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "# 텍스트 상자 그리기\n",
    "orig = image.copy()  # 원본 이미지를 복사\n",
    "if len(indices) > 0:\n",
    "    for i in indices.flatten():\n",
    "        (start_x, start_y, w, h) = boxes[i]\n",
    "        \n",
    "        # 텍스트 영역 그리기\n",
    "        end_x = start_x + w\n",
    "        end_y = start_y + h\n",
    "        cv2.rectangle(orig, (start_x, start_y), (end_x, end_y), (0, 255, 0), 2)\n",
    "\n",
    "# 결과 출력\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Detected Text Areas\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **EAST (Efficient and Accurate Scene Text Detector)**\n",
    "\n",
    "- **설명**: EAST 모델은 이미지 내 텍스트의 위치를 ​​감지하는 데 매우 효과적입니다. 텍스트 경계 상자를 예측하여 텍스트가 있는 영역을 찾습니다.\n",
    "- **장점**: 다양한 텍스트 방향과 크기를 처리할 수 있으며, 빠른 속도로 실시간 처리에 적합합니다.\n",
    "- **사용법**: OpenCV와 함께 사용할 수 있으며, 텍스트 영역을 사각형으로 그려 시각화할 수 있습니다.\n",
    "\n",
    "### 2. **Tesseract OCR**\n",
    "\n",
    "- **설명**: Tesseract는 Google이 지원하는 오픈 소스 OCR 엔진으로, 텍스트 인식에 매우 강력합니다.\n",
    "- **장점**: 다양한 언어를 지원하며, 전처리 과정을 통해 텍스트 인식 정확도를 높일 수 있습니다.\n",
    "- **사용법**: 이미지에서 텍스트를 추출하는 데 간단한 API를 제공합니다.\n",
    "\n",
    "### 3. **EasyOCR**\n",
    "\n",
    "- **설명**: EasyOCR은 PyTorch를 기반으로 한 OCR 라이브러리로, 많은 언어를 지원합니다.\n",
    "- **장점**: 사용자 친화적인 인터페이스와 다양한 언어 지원(영어, 한국어, 중국어, 일본어 등)을 제공합니다.\n",
    "- **사용법**: 설치가 간단하고, 다양한 예제를 통해 쉽게 사용할 수 있습니다.\n",
    "\n",
    "### 4. **PaddleOCR**\n",
    "\n",
    "- **설명**: PaddleOCR은 PaddlePaddle 기반의 OCR 라이브러리로, 실시간 OCR을 지원합니다.\n",
    "- **장점**: 높은 정확도와 빠른 속도를 제공하며, 여러 언어를 지원합니다.\n",
    "- **사용법**: 다양한 모델을 통해 복잡한 이미지에서도 텍스트를 추출할 수 있습니다.\n",
    "\n",
    "### 5. **Deep Text Recognition Model**\n",
    "\n",
    "- **설명**: 이 모델은 텍스트를 인식하는 데 딥러닝을 활용합니다. 이미지의 복잡성을 처리하기 위해 CNN과 RNN을 조합합니다.\n",
    "- **장점**: 복잡한 텍스트 배열에서도 잘 작동합니다.\n",
    "- **사용법**: 학습된 모델을 통해 직접 구현할 수 있으며, 특정 데이터셋에 맞춰 재학습할 수 있습니다.\n",
    "\n",
    "### 6. **Google Vision API**\n",
    "\n",
    "- **설명**: Google Cloud에서 제공하는 API로, 텍스트 감지 및 OCR 기능을 지원합니다.\n",
    "- **장점**: 다양한 텍스트 유형을 인식할 수 있으며, 클라우드 기반으로 작동하여 신뢰성이 높습니다.\n",
    "- **사용법**: API를 통해 손쉽게 텍스트를 추출할 수 있으며, 유료 서비스입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj_poster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
